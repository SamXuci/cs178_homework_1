{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> CS 178: Machine Learning &amp; Data Mining </center>\n",
    "## <center> Homework 1: Due Wednesday 14 January 2026 (11:59 PM)</center>\n",
    "### <center> Version 1.0 (Last Modified: 5 January 2026) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instructions\n",
    "\n",
    "Welcome to CS 178!\n",
    "\n",
    "This homework (and many subsequent ones) will involve data analysis and reporting on methods and results\n",
    "using Python code. You will submit a **single PDF file** that contains everything to Gradescope. This includes any text you wish to include to describe your results, the complete code snippets of how you attempted each problem, any figures that were generated, and scans of any work on paper that you wish to include. It is important that you include enough detail that we know how you solved the problem, since otherwise we will be unable to grade it.\n",
    "\n",
    "\n",
    "Your homeworks will be given to you as Jupyter notebooks containing the problem descriptions and some template code that will help you get started. You are encouraged to modify these starter Jupyter notebooks to complete your assignment and to write your report. You may add additional cells (containing either code or text) as needed. This will help you not only ensure that all of the code for the solutions is included, but also will provide an easy way to export your results to a PDF file (for example, doing *print preview* and *printing to pdf*). I recommend liberal use of Markdown cells to create headers for each problem and sub-problem, explaining your implementation/answers, and including any mathematical equations. For parts of the homework you do on paper, scan it in such that it is legible (there are a number of free Android/iOS scanning apps, if you do not have access to a scanner), and include it as an image in the Jupyter notebook.\n",
    "\n",
    "If you have any questions/concerns about using Jupyter notebooks, ask us on EdD. If you decide not to use Jupyter notebooks, but go with Microsoft Word or Latex to create your PDF file, make sure that all of the answers can be generated from the code snippets included in the document.\n",
    "\n",
    "### Summary of Assignment: 100 total points\n",
    "- Problem 1: Exploring the Iris Dataset (30 points)\n",
    "    - Problem 1.1: Numpy Arrays (5 points)\n",
    "    - Problem 1.2: Feature Statistics (5 points)\n",
    "    - Problem 1.3: Histograms (5 points)\n",
    "    - Problem 1.4: Scatter Plots (10 points)\n",
    "    - Problem 1.5: Vectorization Comparison (5 points)\n",
    "- Problem 2: Linear Regression (35 points)\n",
    "    - Problem 2.1: Visualize Regression Predictions (5 points)\n",
    "    - Problem 2.2: Loss Function (10 points)\n",
    "    - Problem 2.3: Compute Gradient of MSE (10 points)\n",
    "    - Problem 2.4: Gradient Descent Algorithm (10 points)\n",
    "- Problem 3: Model Selection (30 points)\n",
    "    - Problem 3.1: Transform Input Features (5 points)\n",
    "    - Problem 3.2: Fit and Visualize Polynomial Regression (10 points)\n",
    "    - Problem 3.3: Polynomial Degree Selection (15 points)\n",
    "- Statement of Collaboration (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, let's import some libraries that you will make use of in this assigment. Make sure that you run the code cell below in order to import these libraries.\n",
    "\n",
    "**Important: In the code block below, we set `seed=1234`. This is to ensure your code has reproducible results and is important for grading. Do not change this. If you are not using the provided Jupyter notebook, make sure to also set the random seed as below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "# !! Important !! : do not change this\n",
    "seed = 1234\n",
    "np.random.seed(seed)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1: Exploring the Iris Dataset\n",
    "\n",
    "In this problem, you will explore some basic data manipulation and visualizations with the Iris dataset. For every datapoint, we are given several real-valued features which will be used to predict what species of Iris a given flower is. Let's first load in the dataset by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "# Load the features and labels in the Iris dataset\n",
    "iris_X, iris_y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.1 (5 points): Numpy Arrays\n",
    "\n",
    "The variable `iris_X` is a numpy array containing the feature vectors in our dataset, and `iris_y` is a numpy array containing the corresponding labels.\n",
    "\n",
    "- What is the shape of `iris_X` and `iris_y`? ([Hint](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.shape.html))\n",
    "- How many datapoints are in our dataset, and how many features does each datapoint have? \n",
    "- How many different classes (i.e. labels) are there? \n",
    "- Print rows 3, 4, 5, and 6 of the feature matrix and their corresponding labels. Since Python is zero-indexed, we will count our rows starting at zero -- for example, by \\\"row 0\\\" we mean `iris_X[0, :]`, and \\\"row 1\\\" means `iris_X[1, :]`, etc. (Hint: you can do this in two lines of code with slicing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2 (5 points): Feature Statistics\n",
    "\n",
    "Let's compute some statistics about our features. You are allowed to use `numpy` to help you with this problem -- for example, you might find some of the `numpy` functions listed [here](https://numpy.org/doc/stable/reference/routines.statistics.html) or [here](https://numpy.org/doc/stable/reference/routines.math.html) useful.\n",
    "\n",
    "- Compute the mean and standard deviation of each feature.\n",
    "- Compute the minimum and maximum value for each feature.\n",
    "\n",
    "Make sure to print out each of these values, and indicate clearly which value corresponds to which computation.\n",
    "\n",
    "Hint: Numpy has functions that will help you with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3 (5 points):  Feature Histograms\n",
    "- For every feature in `iris_X`, plot a histogram of the values of the feature. \n",
    "- Include a title above each subplot to indicate which feature we are plotting. \n",
    "\n",
    "Some starter code is provided for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 1 row and 4 columns\n",
    "figure, axes = plt.subplots(1, 4, figsize=(12, 3))  \n",
    "\n",
    "### YOUR CODE STARTS HERE ###\n",
    "# Plot a histogram for each feature\n",
    "# Include a title on each subplot\n",
    "\n",
    "### YOUR CODE ENDS HERE  ###\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.4 (10 points): Feature Scatter Plots\n",
    "- For every pair of features in `iris_X`, plot a scatter plot of the feature values, colored according to their labels. For example, plot all data points with $y=0$ as blue, $y=1$ as green, etc. \n",
    "- Include an x-label and a y-label on each subplot to indicate which features we are plotting.\n",
    "\n",
    "For example, you can call the first feature \"Feature 0\", the second feature \"Feature 1\", etc. (Hint: `axes[0, 0].set_xlabel(...)` might help you with the first subplot.)\n",
    "\n",
    "Some starter code is provided for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 4 rows and 4 columns\n",
    "figure, axes = plt.subplots(4, 4, figsize=(8, 8))  \n",
    "\n",
    "### YOUR CODE STARTS HERE ###\n",
    "# Plot a scatter for each feature pair.\n",
    "# Make sure to color the points by their class label.\n",
    "# Include an x-label and a y-label for each subplot.\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE  ###\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.5 (5 points): Vectorization Comparison\n",
    "\n",
    "Part of the appeal of using `numpy` is that it allows for using _vectorized_ operations on your data. Instead of operating on a single value at a time, these allow for performing the same operation on multiple values simultaneously. This can have an immense impact on performance.\n",
    "\n",
    "We will demonstrate this improvement by comparing a vectorized and non-vectorized implementation of calculating the variance of a given feature vector.\n",
    "- Implement the function `variance_loops` to compute the variance of a 1D `numpy` array using loops and indexing into the array. No vectorized operations are allowed (in general, this means no `numpy` commands).\n",
    "- Implement the function `variance_vector` to compute the variance of a 1D `numpy` array using only vectorized operations. No loops or indexing into the array is allowed. Additionally, while `numpy` has `np.var()` implemented, this is not allowed in your implementation. Using `np.mean()` or `np.sum()` is allowed though.\n",
    "\n",
    "Note that `len()` is allowed in both implementations.\n",
    "\n",
    "Recall that the formula for the sample variance of the set of values $x_1, x_2, \\dots, x_n$ is:\n",
    "$$\\text{Var} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\text{ where } \\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i$$\n",
    "\n",
    "Run the cells after the implementation to see the impact that vectorization has. You should see at least a 10x reduction in runtime. When possible, you should always strive for vectorized operations as they will almost always be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_loops(x):\n",
    "    '''x: 1D numpy array'''\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    # Using only native python loops and indexing, calculate variance\n",
    "    # No numpy commands, such as np.sum(), np.mean(), np.var()\n",
    "    \n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    return var\n",
    "\n",
    "def variance_vector(x):\n",
    "    '''x: 1D numpy array'''\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    # Using only numpy commands, such as np.sum() or np.mean(), as well as vectorized operations,\n",
    "    # calculate the variance of `x`.\n",
    "    # No native python loops, indexing, or use of either np.var() or np.std().\n",
    "    \n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to ensure that your implementations for both functions matches the `numpy` implementation. If no exceptions are raised, then you've succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "assert math.isclose(variance_loops(iris_X[:, 0]), iris_X[:, 0].var(ddof=1))\n",
    "assert math.isclose(variance_vector(iris_X[:, 0]), iris_X[:, 0].var(ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells for timing comparisons when computing the variance of the values of Feature 0 in the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit  # JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "variance_loops(iris_X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit  # JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "variance_vector(iris_X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit  # JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "iris_X[:, 0].var(ddof=1)  # numpy's implementation of variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: Linear Regression\n",
    "\n",
    "In this problem, you will implement gradient descent to train a linear model (with a bias unit) on a 1D Dataset by using the mean square error as loss function. **This linear model can be written as $f(x|\\theta) = \\theta_0 + \\theta_1 x$.**\n",
    "\n",
    "Let's first load in the dataset by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "# Generate the inputs and outputs\n",
    "def gen_data(n, seed=seed):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    x = np.linspace(-0.95, 0.95, n)\n",
    "    mean_y = (np.exp(3.75*x) - np.exp(-3.75*x) - 4.0)*0.1\n",
    "    y = mean_y + rng.randn(n)*0.4\n",
    "    return x, y\n",
    "\n",
    "x, y = gen_data(30, seed=seed+1)\n",
    "print(\"X Range: [{:.2f}, {:.2f}], Y Range: [{:.2f}, {:.2f}]\".format(x.min(), x.max(), y.min(), y.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "def plot_data(x, y):\n",
    "    # Make a figure with 1 subplot\n",
    "    fig, axes = plt.subplots()\n",
    "\n",
    "    axes.scatter(x, y)\n",
    "\n",
    "    axes.set_xlabel('x', fontsize=14)\n",
    "    axes.set_ylabel('y', fontsize=14)\n",
    "    \n",
    "    axes.set_xlim(-1.0,1.0)\n",
    "    axes.set_ylim(-4,4)\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "fig, axes = plot_data(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we will focus on the properties of the gradient descent algorithm, rather than generalization\n",
    "performance. Thus we will not create a separate validation dataset, and simply use all available data for training. We will revisit this idea in Problem 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1 (5 points): Visualize regression predictions\n",
    "\n",
    "In our code, the variable `x` is a numpy array containing the feature vector in this 1D dataset, and `y` is a numpy array containing the corresponding target values, which are continuous in the range of roughly $[-4, 4]$.\n",
    "\n",
    "We have a linear model $f(x|\\theta) = \\theta_0 + \\theta_1 x$.\n",
    "\n",
    "- Create a function that, given feature vector `x`, target values `y` and parameters `theta_0`, `theta_1`, plots the data (`x` and `y`) and the predicted regression line that corresponds to the given parameters. \n",
    "\n",
    "Keep in mind that the points **do not** affect the regression line yet, we are only plotting the line in order to visualize it alongside the points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(x, y, theta_0, theta_1): \n",
    "    '''\n",
    "    x:       [# feature vectors], feature values\n",
    "    y:       [# feature vectors], target values\n",
    "    theta_0: the bias parameter of linear model\n",
    "    theta_1: the slope parameter of linear model\n",
    "    '''\n",
    "\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    \n",
    "    ### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "# Display the decision boundary of the linear model governed by the parameters below.\n",
    "theta_0_random, theta_1_random = 0.5, -1\n",
    "\n",
    "plot_regression(x, y, theta_0_random, theta_1_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2 (10 points): Loss function\n",
    "\n",
    "In this problem, we apply the mean squared error (MSE), **which is 1/n of the squared error**, as the loss function for gradient decent. \n",
    "\n",
    "- Write out the mathematical equation of the function of the mean squared error $L(\\theta_0, \\theta_1)$. You can optionally use $\\LaTeX$ in your answer on the jupyter notebook, otherwise, write it down on paper and take a picture or use any other mathematical typesetting tools you have available to you. In order to include an image in jupyter notebook, you save the image in the same directory as the .ipynb file and then write `![caption](image.png)`. The caption may be anything you want, it will only be shown as alt text when hovering over the image.\n",
    "- Implement the function `MSE`.\n",
    "- Report MSE of the linear model defined by `theta_0_random` and `theta_1_random`, across the dataset ($\\textbf{x}$, $\\textbf{y}$);\n",
    "- Visualize the countour plot of MSE in $\\theta$ space, from $\\theta_0 \\in [-2,2]$ and $\\theta_1 \\in [-1,3]$. (Hint: use `np.meshgrid` to create a grid of possible $(\\theta_0, \\theta_1)$ values. Use `ax.contour` to plot the values of $L$ in every value in the grid.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write definition of MSE here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, theta_0, theta_1):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    return y_hat\n",
    "\n",
    "def MSE(x, y, theta_0, theta_1): \n",
    "    '''\n",
    "    x:       [# data points], feature values\n",
    "    y:       [# data points], target values\n",
    "    theta_0: the bias parameter of linear model\n",
    "    theta_1: the slope parameter of linear model\n",
    "    '''\n",
    "    ### YOUR CODE STARTS HERE ###    \n",
    "    \n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "print(MSE(x, y, theta_0_random, theta_1_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "def plot_MSE_contour(x, y):  # Produces a contour plot of the loss landscape\n",
    "    fig, axes = plt.subplots()\n",
    "\n",
    "    Theta_0, Theta_1 = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-1, 3, 100))\n",
    "    \n",
    "    bs_map = []\n",
    "    for theta_0, theta_1 in zip(Theta_0.ravel(), Theta_1.ravel()):\n",
    "        bs_map.append(MSE(x, y, theta_0, theta_1))\n",
    "        \n",
    "    bs_map = np.asarray(bs_map).reshape(100, 100)\n",
    "    \n",
    "    axes.set_xlabel('theta_0', fontsize=14)\n",
    "    axes.set_ylabel('theta_1', fontsize=14)\n",
    "    axes.set_title('MSE', fontsize=18)\n",
    "\n",
    "    im = axes.contour(Theta_0, Theta_1, bs_map, 100)\n",
    "    fig.colorbar(im, ax=axes)\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "fig, axes = plot_MSE_contour(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3 (10 points): Compute the gradient of MSE\n",
    "\n",
    "- Write out the gradient of MSE with respect to $\\theta_0$ and $\\theta_1$, i.e., $\\frac{\\partial}{\\partial \\theta_0} L(\\theta_0, \\theta_1)$ and $\\frac{\\partial}{\\partial \\theta_1} L(\\theta_0, \\theta_1)$. You can use the same typesetting methods as in problem 2.2.\n",
    "- Implement the function `gradient_mse` that computes the gradient of MSE. This function should output a tuple, where the first value is the gradient with respect to $\\theta_0$, and the second values is the gradient with respect to $\\theta_1$.\n",
    "\n",
    "([Here](https://tutorial.math.lamar.edu/classes/calciii/partialderivatives.aspx) is one of many online resources in case you need a refresher on partial derivatives.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write what the gradient of MSE is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_mse(x, y, theta_0, theta_1):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    \n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return grad_0, grad_1  # grad_0 is dL/dtheta_0, grad_1 is dL/dtheta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "grad_0, grad_1 = gradient_mse(x, y, 0.5, -1.0)\n",
    "print(grad_0, grad_1)\n",
    "# We use math.isclose instead of == due to potential floating point precision errors\n",
    "assert math.isclose(grad_0, 1.7177774269022454) \n",
    "assert math.isclose(grad_1, -2.181717782869463)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.4 (10 points): Gradient Descent Algorithm\n",
    "We now have all the prerequisites to implement the gradient descent algorithm. Recall that in gradient descent, we find the gradient (vector pointing towards greatest positive change) of our loss, then increment the parameters by a small amount in the opposite direction of the gradient so that the new parameters have a smaller loss value.\n",
    "\n",
    "Your task for this problem is two-fold:\n",
    "1. Implement the function `gradient_descent`, which takes in:\n",
    "      - the input and output data `x` and `y`, \n",
    "      - the initial values for parameters `theta_0_init` and `theta_1_init`, \n",
    "      - the learning rate `lr` that controls the size of steps taken, \n",
    "      - the number of iterations to execute `max_iters`, \n",
    "      - and finally a flag `display_boundary` that plots intermittent regression plots while training every 20 iterations if it is `True`. \n",
    "      \n",
    "   This function should keep track of all the intermittent values of `theta_0`, `theta_1` and `mse` during training and return all of them at the end in 3 separate lists: `theta_0s`, `theta_1s`, and `mses`. To be clear, the first entry in these lists correspond to the initial settings (i.e., `theta_0s[0] == theta_0_init`) and the last entry corresponds to the final, trained values.\n",
    "2. Train a model using `gradient_descent` and then, using the outputted `mses` create a plot of MSE vs training iteration, to see if the model converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta_0_init, theta_1_init, lr=0.001, max_iters=100, display_boundary=False):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "        \n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return theta_0s, theta_1s, mses  # lists of values of theta_0, theta_1, and mse during each iteration while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "theta_1s, theta_2s, mses = gradient_descent(x, y, theta_0_random, theta_1_random, \n",
    "                                          lr=0.05, max_iters=100, display_boundary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE STARTS HERE ###\n",
    "# Create a plot of MSE vs training iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Model Selection\n",
    "\n",
    "As was seen in the previous problem, a linear model can fit the given data; however, it is far from perfect. It turns out that it is fairly straightforward to adapt a linear model into providing non-linear responses. This is achieved by adding additional inputs that are polynomial transformations of other inputs. For a $p$-degree polynomial, this looks like this:\n",
    "$$f(x|\\theta) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\dots + \\theta_p x^p$$\n",
    "\n",
    "This practice is often referred to as polynomial regression. An important aspect of polynomial regression is that the polynomial degree, $p$, is completely up to us to choose. This is a modeling design decision that must be chosen prior to fitting any data. We call values such as these _hyperparameters_. It may be tempting to just select a very large value for $p$, but this has a good chance of leading to a model that overfits on the given data and does not generalize well outside of the training data.\n",
    "\n",
    "One simple way to select a particular degree (or hyperparameter in general) is to try out multiple different values, and then choose the one that yields the best performance on some held-out set of data. We will refer to this data as the validation set.\n",
    "\n",
    "Let's first load in the validation dataset by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "valid_x, valid_y = gen_data(20, seed=seed-1)  # Use a different seed to get different data than the training split.\n",
    "\n",
    "def plot_data(x, y, valid_x=valid_x, valid_y=valid_y):\n",
    "    # Make a figure with 1 subplot\n",
    "    fig, axes = plt.subplots()\n",
    "\n",
    "    axes.scatter(x, y, label='Training')\n",
    "    if valid_x is not None:\n",
    "        axes.scatter(valid_x, valid_y, label=\"Validation\")\n",
    "        axes.legend()\n",
    "\n",
    "    axes.set_xlabel('x', fontsize=14)\n",
    "    axes.set_ylabel('y', fontsize=14)\n",
    "    \n",
    "    axes.set_xlim(-1.0,1.0)\n",
    "    axes.set_ylim(-4,4)\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "plot_data(x, y, valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1 (5 points): Transform Input Features\n",
    "\n",
    "Just like in problem 2, we will be using the numpy arrays `x` and `y` for 1D feature and target values. What is different now is that we would like to use a polynomial regression model $f(x|\\theta) = \\theta_0 + \\theta_1 x + \\dots + \\theta_p x^p$ where $p$ is the polynomial degree and is known beforehand. To do so, we must first write the function `transform_inputs` that takes in all the values of `x` (with length $n$) that we want to have predictions for and transform them into a $n\\times p$ matrix `X` where each column corresponds to a power of `x`, like so:\n",
    "$$\n",
    "X=\\begin{bmatrix} \n",
    "\\vert & \\vert &  & \\vert \\\\ \n",
    "x & x^2 & \\dots & x^p \\\\\n",
    "\\vert & \\vert & & \\vert\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Feel free to import and use `PolynomialFeatures` from `sklearn.prepocessing` to help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def transform_inputs(x, p):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    \n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return X  # resulting matrix where each column is equal to x^i for i=1,...,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "transform_inputs(np.array([0, 0.5, 1, 2]), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "output = transform_inputs(np.array([0.5, 3]), 4)\n",
    "print(output)\n",
    "np.isclose(\n",
    "    output,\n",
    "    np.array([[0.5, 0.25, 0.125, 0.0625], [3., 9., 27., 81.]])\n",
    ").all()  # Check if all of the elements match between `transform_inputs` and the expected output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.2 (10 points): Fit and Visualize a Polynomial Regression Model\n",
    "Now that we have appropriately transformed the inputs, we can learn the corresponding polynomial regression model. For this problem, we will be relying on the `LinearRegression` class from `sklearn.linear_model` rather than implementing our own fitting procedure from scratch. \n",
    "\n",
    "- Write the function `fit_poly` that takes in 1D arrays `x` and `y`, as well as a desired degree `p`, and returns a fit `LinearRegression` model.\n",
    "- Write the function `plot_poly_regression` that takes in the training data, validation data, and a fit model and plots a visualization of the predicted mean values from the model over the range of input data.\n",
    "- Execute these functions to visualize what a cubic ($p=3$) regression model fit to the training data `x` and `y` looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_poly(x, y, p):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    \n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    return model  # A fit LinearRegression object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_poly_regression(train_x, train_y, valid_x, valid_y, model, p): \n",
    "    '''\n",
    "    train_x:       [# feature vectors], training feature values\n",
    "    train_y:       [# feature vectors], training target values\n",
    "    valid_x:       [# feature vectors], validation feature values\n",
    "    valid_y:       [# feature vectors], validation target values\n",
    "    model:         LinearRegression object\n",
    "    '''\n",
    "    \n",
    "    ### YOUR CODE STARTS HERE\n",
    "    \n",
    "    ### YOUR CODE ENDS HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN - DO NOT EDIT THIS CODE BLOCK\n",
    "cubic_model = fit_poly(x, y, p=3)\n",
    "plot_poly_regression(x, y, valid_x, valid_y, cubic_model, p=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.3 (15 points): Polynomial Degree Selection\n",
    "\n",
    "- Choose a value $p$ such that the associated model **underfits** the data. Demonstrate this with a plot.\n",
    "- Choose a value $p$ such that the associated model **overfits** the data. Demonstrate this with a plot.\n",
    "- Using the validation set, find the optimal value of $p$ that leads to the lowest MSE. Consider values of $p$ from $p=1$ up to $p=20$. Report the lowest held-out MSE score, the associated degree $p$, and plot the resulting model fit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Statement of Collaboration (5 points)\n",
    "\n",
    "It is **mandatory** to include a Statement of Collaboration in each submission, with respect to the guidelines below. Include the names of everyone involved in the discussions (especially in-person ones), and what was discussed.\n",
    "\n",
    "All students are required to follow the academic honesty guidelines posted on the course website. For\n",
    "programming assignments, in particular, I encourage the students to organize (perhaps using EdD) to\n",
    "discuss the task descriptions, requirements, bugs in my code, and the relevant technical content before they start\n",
    "working on it. However, you should not discuss the specific solutions, and, as a guiding principle, you are not\n",
    "allowed to take anything written or drawn away from these discussions (i.e. no photographs of the blackboard,\n",
    "written notes, referring to EdD, etc.). Especially after you have started working on the assignment, try\n",
    "to restrict the discussion to EdD as much as possible, so that there is no doubt as to the extent of your\n",
    "collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
